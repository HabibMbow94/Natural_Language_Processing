{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Text Preprocessing In Natural Language Processing </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Installs packages </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Using cached pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.9.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (2.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: packaging in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Import Libraries </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/habibmbow/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/habibmbow/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/habibmbow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks.[1] It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.[3] A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[5][6] Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.[7][8] In its application across business problems, machine learning is also referred to as predictive analytics.\n"
     ]
    }
   ],
   "source": [
    "text = \"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks.[1] It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.[3] A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[5][6] Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.[7][8] In its application across business problems, machine learning is also referred to as predictive analytics.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Machine learning ML is a field of inquiry devoted to understanding and building methods that learn that is methods that leverage data to improve performance on some set of tasks1 It is seen as a part of artificial intelligence Machine learning algorithms build a model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to do so2 Machine learning algorithms are used in a wide variety of applications such as in medicine email filtering speech recognition and computer vision where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks3 A subset of machine learning is closely related to computational statistics which focuses on making predictions using computers but not all machine learning is statistical learning The study of mathematical optimization delivers methods theory and application domains to the field of machine learning Data mining is a related field of study focusing on exploratory data analysis through unsupervised learning56 Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain78 In its application across business problems machine learning is also referred to as predictive analytics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1290"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    txt_no_punct = \" \"\n",
    "    for ele in text:\n",
    "        if ele not in string.punctuation:\n",
    "            txt_no_punct += ele\n",
    "    return txt_no_punct\n",
    "\n",
    "print(remove_punctuation(text))\n",
    "len(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning ML is a field of inquiry devoted to understanding and building methods that learn that is methods that leverage data to improve performance on some set of tasks1 It is seen as a part of artificial intelligence Machine learning algorithms build a model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to do so2 Machine learning algorithms are used in a wide variety of applications such as in medicine email filtering speech recognition and computer vision where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks3 A subset of machine learning is closely related to computational statistics which focuses on making predictions using computers but not all machine learning is statistical learning The study of mathematical optimization delivers methods theory and application domains to the field of machine learning Data mining is a related field of study focusing on exploratory data analysis through unsupervised learning56 Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain78 In its application across business problems machine learning is also referred to as predictive analytics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(txt):\n",
    "    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopunct\n",
    "print(remove_punctuation(text))\n",
    "len(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289\n",
      "Machine learning ML is a field of inquiry devoted to understanding and building methods that learn that is methods that leverage data to improve performance on some set of tasks1 It is seen as a part of artificial intelligence Machine learning algorithms build a model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to do so2 Machine learning algorithms are used in a wide variety of applications such as in medicine email filtering speech recognition and computer vision where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks3 A subset of machine learning is closely related to computational statistics which focuses on making predictions using computers but not all machine learning is statistical learning The study of mathematical optimization delivers methods theory and application domains to the field of machine learning Data mining is a related field of study focusing on exploratory data analysis through unsupervised learning56 Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain78 In its application across business problems machine learning is also referred to as predictive analytics\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuations in string Using regex\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    res = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return res\n",
    " \n",
    "# printing result\n",
    "text = remove_punctuation(text)\n",
    "print(len(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization is the process of converting the text into smaller units or segments known as tokens. The tokens can be sentences, words, or subwords (n-gram characters). These tokens aid in the comprehension of the context or the development of the NLP model. By evaluating the sequence of words, tokenization aids in deciphering the meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Tokenization using Python’s split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'ML', 'is', 'a', 'field', 'of', 'inquiry', 'devoted', 'to', 'understanding', 'and', 'building', 'methods', 'that', 'learn', 'that', 'is', 'methods', 'that', 'leverage', 'data', 'to', 'improve', 'performance', 'on', 'some', 'set', 'of', 'tasks1', 'It', 'is', 'seen', 'as', 'a', 'part', 'of', 'artificial', 'intelligence', 'Machine', 'learning', 'algorithms', 'build', 'a', 'model', 'based', 'on', 'sample', 'data', 'known', 'as', 'training', 'data', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so2', 'Machine', 'learning', 'algorithms', 'are', 'used', 'in', 'a', 'wide', 'variety', 'of', 'applications', 'such', 'as', 'in', 'medicine', 'email', 'filtering', 'speech', 'recognition', 'and', 'computer', 'vision', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks3', 'A', 'subset', 'of', 'machine', 'learning', 'is', 'closely', 'related', 'to', 'computational', 'statistics', 'which', 'focuses', 'on', 'making', 'predictions', 'using', 'computers', 'but', 'not', 'all', 'machine', 'learning', 'is', 'statistical', 'learning', 'The', 'study', 'of', 'mathematical', 'optimization', 'delivers', 'methods', 'theory', 'and', 'application', 'domains', 'to', 'the', 'field', 'of', 'machine', 'learning', 'Data', 'mining', 'is', 'a', 'related', 'field', 'of', 'study', 'focusing', 'on', 'exploratory', 'data', 'analysis', 'through', 'unsupervised', 'learning56', 'Some', 'implementations', 'of', 'machine', 'learning', 'use', 'data', 'and', 'neural', 'networks', 'in', 'a', 'way', 'that', 'mimics', 'the', 'working', 'of', 'a', 'biological', 'brain78', 'In', 'its', 'application', 'across', 'business', 'problems', 'machine', 'learning', 'is', 'also', 'referred', 'to', 'as', 'predictive', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "# The word_tokenize() function can also be used to split the sentences into individual words if necessary.\n",
    "def tokenize_split(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "print(tokenize_split(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Tokenization using Regular Expressions (RegEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'ML', 'is', 'a', 'field', 'of', 'inquiry', 'devoted', 'to', 'understanding', 'and', 'building', 'methods', 'that', 'learn', 'that', 'is', 'methods', 'that', 'leverage', 'data', 'to', 'improve', 'performance', 'on', 'some', 'set', 'of', 'tasks1', 'It', 'is', 'seen', 'as', 'a', 'part', 'of', 'artificial', 'intelligence', 'Machine', 'learning', 'algorithms', 'build', 'a', 'model', 'based', 'on', 'sample', 'data', 'known', 'as', 'training', 'data', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so2', 'Machine', 'learning', 'algorithms', 'are', 'used', 'in', 'a', 'wide', 'variety', 'of', 'applications', 'such', 'as', 'in', 'medicine', 'email', 'filtering', 'speech', 'recognition', 'and', 'computer', 'vision', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks3', 'A', 'subset', 'of', 'machine', 'learning', 'is', 'closely', 'related', 'to', 'computational', 'statistics', 'which', 'focuses', 'on', 'making', 'predictions', 'using', 'computers', 'but', 'not', 'all', 'machine', 'learning', 'is', 'statistical', 'learning', 'The', 'study', 'of', 'mathematical', 'optimization', 'delivers', 'methods', 'theory', 'and', 'application', 'domains', 'to', 'the', 'field', 'of', 'machine', 'learning', 'Data', 'mining', 'is', 'a', 'related', 'field', 'of', 'study', 'focusing', 'on', 'exploratory', 'data', 'analysis', 'through', 'unsupervised', 'learning56', 'Some', 'implementations', 'of', 'machine', 'learning', 'use', 'data', 'and', 'neural', 'networks', 'in', 'a', 'way', 'that', 'mimics', 'the', 'working', 'of', 'a', 'biological', 'brain78', 'In', 'its', 'application', 'across', 'business', 'problems', 'machine', 'learning', 'is', 'also', 'referred', 'to', 'as', 'predictive', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_re(text):\n",
    "    tokens = re.findall(\"[\\w]+\", text)\n",
    "    return tokens\n",
    "print(tokenize_re(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'ML', 'is', 'a', 'field', 'of', 'inquiry', 'devoted', 'to', 'understanding', 'and', 'building', 'methods', 'that', 'learn', 'that', 'is', 'methods', 'that', 'leverage', 'data', 'to', 'improve', 'performance', 'on', 'some', 'set', 'of', 'tasks1', 'It', 'is', 'seen', 'as', 'a', 'part', 'of', 'artificial', 'intelligence', 'Machine', 'learning', 'algorithms', 'build', 'a', 'model', 'based', 'on', 'sample', 'data', 'known', 'as', 'training', 'data', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so2', 'Machine', 'learning', 'algorithms', 'are', 'used', 'in', 'a', 'wide', 'variety', 'of', 'applications', 'such', 'as', 'in', 'medicine', 'email', 'filtering', 'speech', 'recognition', 'and', 'computer', 'vision', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks3', 'A', 'subset', 'of', 'machine', 'learning', 'is', 'closely', 'related', 'to', 'computational', 'statistics', 'which', 'focuses', 'on', 'making', 'predictions', 'using', 'computers', 'but', 'not', 'all', 'machine', 'learning', 'is', 'statistical', 'learning', 'The', 'study', 'of', 'mathematical', 'optimization', 'delivers', 'methods', 'theory', 'and', 'application', 'domains', 'to', 'the', 'field', 'of', 'machine', 'learning', 'Data', 'mining', 'is', 'a', 'related', 'field', 'of', 'study', 'focusing', 'on', 'exploratory', 'data', 'analysis', 'through', 'unsupervised', 'learning56', 'Some', 'implementations', 'of', 'machine', 'learning', 'use', 'data', 'and', 'neural', 'networks', 'in', 'a', 'way', 'that', 'mimics', 'the', 'working', 'of', 'a', 'biological', 'brain78', 'In', 'its', 'application', 'across', 'business', 'problems', 'machine', 'learning', 'is', 'also', 'referred', 'to', 'as', 'predictive', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text_tokenize = word_tokenize(text)\n",
    "\n",
    "print(text_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Tokenization Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Now we will process above text using 'nlp' object. Which is use to create documents with linguistic annotations and various nlp properties\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Above step has already tokenized our text but its in doc format, so lets write fo loop to create list of it\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Tokenization using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "tokens = text_to_word_sequence(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Tokenization using Gensim\n",
    "\n",
    "* Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n",
    "* We are going to use tokenize() from gensim.utility class for word tokenization.\n",
    "* Unlike other libraries Gensim has separate method split_sentences() from class gensim.summarization.textcleaner for sentence tokenization.\n",
    "* Syntx to install Gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'ML', 'is', 'a', 'field', 'of', 'inquiry', 'devoted', 'to', 'understanding', 'and', 'building', 'methods', 'that', 'learn', 'that', 'is', 'methods', 'that', 'leverage', 'data', 'to', 'improve', 'performance', 'on', 'some', 'set', 'of', 'tasks', 'It', 'is', 'seen', 'as', 'a', 'part', 'of', 'artificial', 'intelligence', 'Machine', 'learning', 'algorithms', 'build', 'a', 'model', 'based', 'on', 'sample', 'data', 'known', 'as', 'training', 'data', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so', 'Machine', 'learning', 'algorithms', 'are', 'used', 'in', 'a', 'wide', 'variety', 'of', 'applications', 'such', 'as', 'in', 'medicine', 'email', 'filtering', 'speech', 'recognition', 'and', 'computer', 'vision', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks', 'A', 'subset', 'of', 'machine', 'learning', 'is', 'closely', 'related', 'to', 'computational', 'statistics', 'which', 'focuses', 'on', 'making', 'predictions', 'using', 'computers', 'but', 'not', 'all', 'machine', 'learning', 'is', 'statistical', 'learning', 'The', 'study', 'of', 'mathematical', 'optimization', 'delivers', 'methods', 'theory', 'and', 'application', 'domains', 'to', 'the', 'field', 'of', 'machine', 'learning', 'Data', 'mining', 'is', 'a', 'related', 'field', 'of', 'study', 'focusing', 'on', 'exploratory', 'data', 'analysis', 'through', 'unsupervised', 'learning', 'Some', 'implementations', 'of', 'machine', 'learning', 'use', 'data', 'and', 'neural', 'networks', 'in', 'a', 'way', 'that', 'mimics', 'the', 'working', 'of', 'a', 'biological', 'brain', 'In', 'its', 'application', 'across', 'business', 'problems', 'machine', 'learning', 'is', 'also', 'referred', 'to', 'as', 'predictive', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "tokens = list(tokenize(text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/habibmbow/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "#let's check how many stop words does english contains.\n",
    "en_stopwords=nltk.corpus.stopwords.words('english')\n",
    "print(len(en_stopwords))\n",
    "#and let's see first 20 stop words for english\n",
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Remove stopwords using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'ML', 'field', 'inquiry', 'devoted', 'understanding', 'building', 'methods', 'learn', 'methods', 'leverage', 'data', 'improve', 'performance', 'set', 'tasks1', 'It', 'seen', 'part', 'artificial', 'intelligence', 'Machine', 'learning', 'algorithms', 'build', 'model', 'based', 'sample', 'data', 'known', 'training', 'data', 'order', 'make', 'predictions', 'decisions', 'without', 'explicitly', 'programmed', 'so2', 'Machine', 'learning', 'algorithms', 'used', 'wide', 'variety', 'applications', 'medicine', 'email', 'filtering', 'speech', 'recognition', 'computer', 'vision', 'difficult', 'unfeasible', 'develop', 'conventional', 'algorithms', 'perform', 'needed', 'tasks3', 'A', 'subset', 'machine', 'learning', 'closely', 'related', 'computational', 'statistics', 'focuses', 'making', 'predictions', 'using', 'computers', 'machine', 'learning', 'statistical', 'learning', 'The', 'study', 'mathematical', 'optimization', 'delivers', 'methods', 'theory', 'application', 'domains', 'field', 'machine', 'learning', 'Data', 'mining', 'related', 'field', 'study', 'focusing', 'exploratory', 'data', 'analysis', 'unsupervised', 'learning56', 'Some', 'implementations', 'machine', 'learning', 'use', 'data', 'neural', 'networks', 'way', 'mimics', 'working', 'biological', 'brain78', 'In', 'application', 'across', 'business', 'problems', 'machine', 'learning', 'also', 'referred', 'predictive', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(txt_tokenize):\n",
    "    text_remove_stopwords = []\n",
    "    for word in text_tokenize:\n",
    "        if word not in en_stopwords:\n",
    "            text_remove_stopwords.append(word)\n",
    "    return text_remove_stopwords\n",
    "text_remove_stopwords = remove_stopwords(text_tokenize)\n",
    "print(text_remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stemming\n",
    "\n",
    "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat. Search engines use stemming for indexing the words. That’s why rather than storing all forms of a word, a search engine can store only the stems. In this way, stemming reduces the size of the index and increases retrieval accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Stemming using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machin', 'learn', 'ml', 'field', 'inquiri', 'devot', 'understand', 'build', 'method', 'learn', 'method', 'leverag', 'data', 'improv', 'perform', 'set', 'tasks1', 'it', 'seen', 'part', 'artifici', 'intellig', 'machin', 'learn', 'algorithm', 'build', 'model', 'base', 'sampl', 'data', 'known', 'train', 'data', 'order', 'make', 'predict', 'decis', 'without', 'explicitli', 'program', 'so2', 'machin', 'learn', 'algorithm', 'use', 'wide', 'varieti', 'applic', 'medicin', 'email', 'filter', 'speech', 'recognit', 'comput', 'vision', 'difficult', 'unfeas', 'develop', 'convent', 'algorithm', 'perform', 'need', 'tasks3', 'a', 'subset', 'machin', 'learn', 'close', 'relat', 'comput', 'statist', 'focus', 'make', 'predict', 'use', 'comput', 'machin', 'learn', 'statist', 'learn', 'the', 'studi', 'mathemat', 'optim', 'deliv', 'method', 'theori', 'applic', 'domain', 'field', 'machin', 'learn', 'data', 'mine', 'relat', 'field', 'studi', 'focus', 'exploratori', 'data', 'analysi', 'unsupervis', 'learning56', 'some', 'implement', 'machin', 'learn', 'use', 'data', 'neural', 'network', 'way', 'mimic', 'work', 'biolog', 'brain78', 'in', 'applic', 'across', 'busi', 'problem', 'machin', 'learn', 'also', 'refer', 'predict', 'analyt']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing sentences\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def text_stemming(text):\n",
    "    text_stemming = []\n",
    "    for word in text:\n",
    "        word_stem = ps.stem(word)\n",
    "        text_stemming.append(word_stem)\n",
    "    return text_stemming\n",
    "text_stemming = text_stemming(text_remove_stopwords)\n",
    "print(text_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lemmatization\n",
    "Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/habibmbow/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/habibmbow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machin', 'learn', 'ml', 'field', 'inquiri', 'devot', 'understand', 'build', 'method', 'learn', 'method', 'leverag', 'data', 'improv', 'perform', 'set', 'tasks1', 'it', 'seen', 'part', 'artifici', 'intellig', 'machin', 'learn', 'algorithm', 'build', 'model', 'base', 'sampl', 'data', 'known', 'train', 'data', 'order', 'make', 'predict', 'decis', 'without', 'explicitli', 'program', 'so2', 'machin', 'learn', 'algorithm', 'use', 'wide', 'varieti', 'applic', 'medicin', 'email', 'filter', 'speech', 'recognit', 'comput', 'vision', 'difficult', 'unfeas', 'develop', 'convent', 'algorithm', 'perform', 'need', 'tasks3', 'a', 'subset', 'machin', 'learn', 'close', 'relat', 'comput', 'statist', 'focus', 'make', 'predict', 'use', 'comput', 'machin', 'learn', 'statist', 'learn', 'the', 'studi', 'mathemat', 'optim', 'deliv', 'method', 'theori', 'applic', 'domain', 'field', 'machin', 'learn', 'data', 'mine', 'relat', 'field', 'studi', 'focus', 'exploratori', 'data', 'analysi', 'unsupervis', 'learning56', 'some', 'implement', 'machin', 'learn', 'use', 'data', 'neural', 'network', 'way', 'mimic', 'work', 'biolog', 'brain78', 'in', 'applic', 'across', 'busi', 'problem', 'machin', 'learn', 'also', 'refer', 'predict', 'analyt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def text_lemmatize(text):\n",
    "    txt_lemmatize = []\n",
    "    for word in text:      \n",
    "        txt_lem = lemmatizer.lemmatize(word)\n",
    "        txt_lemmatize.append(txt_lem)\n",
    "    return txt_lemmatize\n",
    "text_lemmatize = text_lemmatize(text_stemming)\n",
    "# Lemmatize Single Word\n",
    "print(text_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/habibmbow/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words ... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Creating the bag of words ... \\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(analyzer = \"word\" , \\\n",
    "#                              tokenizer = None , \\\n",
    "#                              preprocessor = None , \\\n",
    "#                              stop_words = None , \\\n",
    "#                              max_features = 5000)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\" , \n",
    "                             max_features = 5000)\n",
    "\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(text_lemmatize)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a function which will return a list of all words in a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_unique_word(text) :\n",
    " \"\"\"\n",
    "  \"\"\"\n",
    " unique_word = set()\n",
    " for word in text:\n",
    "   unique_word_doc = set(text)\n",
    "   if word in unique_word_doc:\n",
    "     unique_word.add(word)\n",
    " return list(unique_word)\n",
    "\n",
    "words= get_unique_word(text_lemmatize)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the one hote function encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (127, 1), indices imply (127, 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d3/5nbyn1mn76d616kqs6f5t83m0000gn/T/ipykernel_54621/960440336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_lemmatize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/d3/5nbyn1mn76d616kqs6f5t83m0000gn/T/ipykernel_54621/960440336.py\u001b[0m in \u001b[0;36mone_hot_encoding\u001b[0;34m(text, list_of_all_words)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_of_all_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    735\u001b[0m                     )\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                     mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    738\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (127, 1), indices imply (127, 85)"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(text_data_frame, list_of_all_words):\n",
    "    \"\"\"\n",
    "    This function take imput a text dataframe and return a binary trasformation \n",
    "    \"\"\"\n",
    "    mat = []\n",
    "    for doc in text_data_frame.review:\n",
    "        doc_vec = []\n",
    "        for word in list_of_all_words:\n",
    "            value = 1 if word in doc else 0\n",
    "            doc_vec.append(value)\n",
    "        mat.append(doc_vec)\n",
    "    df = pd.DataFrame(data = mat, columns = list_of_all_words)\n",
    "    return df\n",
    "\n",
    "one_hot_encoding(dataframe, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(text, unique_word):\n",
    "    n_docs = len(text) #·Number of documents in the dataframe\n",
    "\n",
    "    n_words = len(unique_word) # Number of unique words in the dataframe\n",
    "\n",
    "    df_tf_idf = pd.DataFrame(np.zeros((n_docs, n_words)), columns=unique_word) #create a dataframe by the number of documents in the dataframe and the word, and use that information to compute the TF-IDF:\n",
    "\n",
    "    tf = {} # dictionnary vide contains TF\n",
    "    idf = {} # dictionnary vide contains IDF\n",
    "    \n",
    "    # Compute the Term Frequency (TF)\n",
    "    for word in text:\n",
    "        tf[word] = word.count(word)/len(text)\n",
    "    \n",
    "    # Compute the Inverse Document Frequency (IDF)\n",
    "    for w in unique_word:\n",
    "        k = 0    # number of documents in the dataframe that contain this word\n",
    "        \n",
    "        if w in text:\n",
    "            k += 1\n",
    "                \n",
    "        idf[w] =  np.log10(n_docs / k)\n",
    "    # Compute the Term Frequency Inverse Document Frequency (TF_ IDF)\n",
    "    \n",
    "    for word in unique_word:\n",
    "            df_tf_idf[word] = tf[word]*idf[word] \n",
    "    df_tf_idf\n",
    "    return df_tf_idf\n",
    "\n",
    "text_tf_idf = tf_idf(text_lemmatize, words)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a46bd6e24c1f53fffd0c2fe37a3e93b2b8ebf08f8d9dc31c4646ffc9359c678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
